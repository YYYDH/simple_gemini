import streamlit as st
import base64
from google.generativeai import GenerativeModel, configure

# ------------------------------
# é¡µé¢ & ä¾§è¾¹æ é…ç½®
# ------------------------------
st.set_page_config(page_title="Gemini AI èŠå¤©", page_icon="ğŸ¤–", layout="wide")

st.title("ğŸ¤– Gemini AI èŠå¤©åŠ©æ‰‹")
st.caption("åŸºäº Google Gemini API çš„ç®€å•èŠå¤©å·¥å…· â€” ä¿æŒ chat_inputï¼ˆç½®åº• + è‡ªåŠ¨é«˜åº¦ï¼‰ï¼Œå³ä¾§æ·»åŠ é™„ä»¶æŒ‰é’®")

with st.sidebar:
    st.header("ğŸ”§ é…ç½®")
    api_key = st.text_input("è¯·è¾“å…¥ä½ çš„ Google Gemini API Key", type="password")
    st.caption("API Key å¯ä» Google AI Studio è·å–")

    models = [
        "gemini-2.5-pro",
        "gemini-2.5-pro-latest",
        "gemini-1.5-pro",
        "gemini-1.5-flash",
        "gemini-pro"
    ]
    selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹", models, index=0)

    st.write("---")
    send_file_contents = st.checkbox("å‘é€æ–‡ä»¶å†…å®¹ç»™ Geminiï¼ˆå°†æŠŠæ–‡ä»¶ base64 ç¼–ç éšæ¶ˆæ¯å‘é€ï¼‰", value=False)
    st.caption("å…³é—­åˆ™ä»…ä¿å­˜æ–‡ä»¶åä½œä¸ºå…ƒæ•°æ®ï¼›å¼€å¯å°†æŠŠå°æ–‡ä»¶å†…å®¹éšæ¶ˆæ¯å‘é€ï¼ˆæ³¨æ„éšç§ä¸å¤§å°ï¼‰")

    if st.button("ğŸ—‘ï¸ æ¸…ç©ºèŠå¤©è®°å½•"):
        st.session_state.pop("messages", None)
        st.session_state.pop("pending_attachments", None)
        st.experimental_rerun()

# ------------------------------
# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
# ------------------------------
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# pending_attachments å­˜å‚¨ä¸º list of dict: {"name": ..., "data": bytes, "type": mime}
if "pending_attachments" not in st.session_state:
    st.session_state["pending_attachments"] = []

# ------------------------------
# æ˜¾ç¤ºå†å²èŠå¤©è®°å½•
# ------------------------------
for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        content = msg.get("content", "")
        # å¦‚æœæœ‰é™„ä»¶å…ƒæ•°æ®ï¼Œæ˜¾ç¤ºæ–‡ä»¶åå¹¶æä¾›ä¸‹è½½æŒ‰é’®
        attachments = msg.get("attachments", [])
        if attachments:
            # æ˜¾ç¤ºæ­£æ–‡ + é™„ä»¶æ¸…å•
            st.markdown(content)
            st.markdown("**é™„ä»¶ï¼š**")
            for idx, att in enumerate(attachments):
                att_name = att.get("name")
                att_bytes = att.get("data")  # bytes or None
                if att_bytes:
                    st.download_button(
                        label=f"ä¸‹è½½ {att_name}",
                        data=att_bytes,
                        file_name=att_name,
                        key=f"dl_{msg['role']}_{idx}_{att_name}"
                    )
                else:
                    st.markdown(f"- {att_name}")
        else:
            st.markdown(content)

st.markdown("---")

# ------------------------------
# æµ®åŠ¨é™„ä»¶ä¸Šä¼ æŒ‰é’®ï¼ˆå®šä½åˆ°èŠå¤©è¾“å…¥æ¡†é™„è¿‘ï¼‰
# ------------------------------
# æˆ‘ä»¬ç”¨ä¸€ä¸ªå¯è§ä½†å°çš„ file_uploaderï¼Œç„¶åç”¨ CSS å®šä½å®ƒåˆ°å³ä¸‹ï¼ˆé è¿‘ chat_inputï¼‰ã€‚
# (æ³¨æ„ï¼šä¸åŒ Streamlit ç‰ˆæœ¬ çš„ DOM ç»†èŠ‚ä¼šæœ‰å·®å¼‚ï¼Œå¿…è¦æ—¶è°ƒæ•´ right/bottom å€¼)
file_uploader = st.file_uploader(
    label="",
    accept_multiple_files=True,
    key="floating_uploader",
    label_visibility="collapsed"
)

st.markdown(
    """
    <style>
    /* å°† file_uploader å›ºå®šåˆ°å³ä¸‹é è¿‘ chat_input çš„ä½ç½®ï¼Œè°ƒæ•´ right/bottom ä»¥é€‚é…ä½ çš„ä¸»é¢˜ */
    div[data-testid="stFileUploader"] {
        position: fixed;
        right: 160px;   /* æ ¹æ®ä½ çš„é¡µé¢è°ƒæ•´ï¼Œè®©å®ƒç´§è´´å‘é€æŒ‰é’® */
        bottom: 92px;   /* å¤§è‡´å€¼ï¼šä½¿æŒ‰é’®åœ¨ chat_input çš„å·¦ä¾§/ä¸Šæ–¹ */
        z-index: 9999;
        width: 44px;    /* æ§åˆ¶å¯è§†å¤§å° */
        height: 44px;
        overflow: visible;
    }
    /* éšè—æ–‡ä»¶ä¸Šä¼ é»˜è®¤æ–‡å­—ï¼ˆä¿æŒå›¾æ ‡æˆ–å°è¾“å…¥ï¼‰*/
    div[data-testid="stFileUploader"] > label {
        display: none;
    }
    /* è°ƒæ•´æ–‡ä»¶é€‰æ‹©æŒ‰é’®çš„å®é™…æ ·å¼ï¼ˆå¯èƒ½éœ€è¦æ ¹æ® streamlit ç‰ˆæœ¬å¾®è°ƒï¼‰ */
    div[data-testid="stFileUploader"] > div {
        padding: 0;
        margin: 0;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# å½“ç”¨æˆ·åœ¨ file_uploader é€‰æ‹©äº†æ–‡ä»¶ï¼Œæˆ‘ä»¬æŠŠæ–‡ä»¶ä¿å­˜åœ¨ session_state.pending_attachmentsï¼ˆbytesï¼‰ï¼Œä½†ä¸è‡ªåŠ¨å‘é€
if file_uploader:
    # file_uploader å¯èƒ½æ˜¯åˆ—è¡¨ï¼ˆaccept_multiple_files=Trueï¼‰
    files = file_uploader if isinstance(file_uploader, list) else [file_uploader]
    new_added = []
    for f in files:
        # avoid duplicate if same file object already stored (by name+size)
        try:
            f_bytes = f.read()
        except Exception:
            f_bytes = None
        fingerprint = (f.name, len(f_bytes) if f_bytes is not None else -1)
        existed = False
        for existing in st.session_state["pending_attachments"]:
            if (existing.get("name"), existing.get("size")) == fingerprint:
                existed = True
                break
        if not existed:
            st.session_state["pending_attachments"].append({
                "name": f.name,
                "data": f_bytes,
                "size": len(f_bytes) if f_bytes is not None else None,
                "type": getattr(f, "type", None)
            })
            new_added.append(f.name)
    if new_added:
        st.toast_text = f"å·²æ·»åŠ é™„ä»¶: {', '.join(new_added)}"  # ä»…ä½œåé¦ˆï¼ˆéæ ‡å‡† APIï¼›è‹¥æ— æ•ˆå¯åˆ é™¤ï¼‰

# æ˜¾ç¤ºå½“å‰å¾…å‘é€é™„ä»¶ï¼ˆå¹¶æä¾›æ¸…é™¤æŒ‰é’®ï¼‰
if st.session_state["pending_attachments"]:
    cols = st.columns([0.9, 0.1])
    pending_names = ", ".join([p["name"] for p in st.session_state["pending_attachments"]])
    cols[0].markdown(f"**å¾…å‘é€é™„ä»¶ï¼š** {pending_names}")
    if cols[1].button("âœ– æ¸…é™¤é™„ä»¶"):
        st.session_state["pending_attachments"] = []

# ------------------------------
# èŠå¤©è¾“å…¥ï¼ˆä¿æŒ chat_input å›ºæœ‰ç‰¹æ€§ï¼‰
# ------------------------------
if api_key:
    configure(api_key=api_key)
    model = GenerativeModel(selected_model)

    # å½“ç”¨æˆ·æäº¤è¾“å…¥ï¼ˆchat_input ä¼šå›ºå®šåœ¨é¡µé¢åº•éƒ¨å¹¶è‡ªåŠ¨è°ƒæ•´é«˜åº¦ï¼‰
    user_input = st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜...")
    if user_input:
        # æ„é€ é™„ä»¶å…ƒæ•°æ®ï¼ˆé»˜è®¤åªå‘é€åå­—ï¼›å¦‚ä¾§è¾¹æ å‹¾é€‰åˆ™åŒæ—¶é™„å¸¦ base64 å†…å®¹ï¼‰
        attachments_payload = []
        for att in st.session_state.get("pending_attachments", []):
            payload_item = {"name": att["name"]}
            if send_file_contents and att.get("data") is not None:
                # base64 encode bytes to string
                b64 = base64.b64encode(att["data"]).decode("utf-8")
                payload_item["data_base64"] = b64
                payload_item["size"] = att.get("size")
                payload_item["type"] = att.get("type")
            attachments_payload.append(payload_item)

        # æŠŠç”¨æˆ·æ¶ˆæ¯åŠ å…¥ä¼šè¯ï¼ˆåŒ…æ‹¬é™„ä»¶å…ƒæ•°æ®ï¼‰
        st.session_state["messages"].append({
            "role": "user",
            "content": user_input,
            "attachments": attachments_payload
        })
        with st.chat_message("user"):
            display_text = user_input
            if attachments_payload:
                display_text += "\n\n**é™„ä»¶:** " + ", ".join([a["name"] for a in attachments_payload])
            st.markdown(display_text)

        # æ¸…é™¤ pendingï¼ˆå·²åŠ å…¥æ¶ˆæ¯çš„é™„ä»¶ä» pending ä¸­ç§»é™¤ï¼‰
        st.session_state["pending_attachments"] = []

        # ------------------------------
        # è°ƒç”¨ Gemini ç”Ÿæˆå›å¤ï¼ˆå°è¯•æµå¼ï¼Œå¦‚æœ SDK ä¸æ”¯æŒåˆ™å›é€€ï¼‰
        # ------------------------------
        with st.chat_message("assistant"):
            placeholder = st.empty()
            full_response = ""

            try:
                # ä¼˜å…ˆå°è¯•æµå¼æ¥å£ï¼ˆè‹¥ä½ çš„ SDK æ”¯æŒ stream=Trueï¼‰
                response = model.generate_content(user_input, stream=True)
                # è‹¥è¿”å›å¯è¿­ä»£æµï¼Œåˆ™é€å—æ‹¼æ¥
                try:
                    for chunk in response:
                        # chunk ç»“æ„å¯èƒ½ä¸åŒï¼šå°è¯•å¤šç§è®¿é—®æ–¹å¼
                        text_piece = None
                        if hasattr(chunk, "text"):
                            text_piece = getattr(chunk, "text")
                        elif isinstance(chunk, dict):
                            text_piece = chunk.get("text") or chunk.get("output_text") or str(chunk)
                        else:
                            text_piece = str(chunk)
                        if text_piece:
                            full_response += text_piece
                            placeholder.markdown(full_response + "â–Œ")
                    placeholder.markdown(full_response)
                except TypeError:
                    # response ä¸èƒ½è¢«è¿­ä»£ï¼Œè§†ä¸ºéæµå¼ç»“æœ
                    # ç»§ç»­åé¢ä½œä¸ºéæµå¼å¤„ç†
                    raise Exception("stream returned non-iterable")
            except Exception:
                # å›é€€åˆ°éæµå¼è°ƒç”¨ï¼ˆå…¼å®¹æ—§ SDKï¼‰
                try:
                    response = model.generate_content(user_input)
                    # å°è¯•æå–æ–‡æœ¬å­—æ®µ
                    text = None
                    if hasattr(response, "text"):
                        text = getattr(response, "text")
                    elif isinstance(response, dict):
                        # å¸¸è§ç»“æ„å°è¯•
                        text = response.get("text") or response.get("output_text")
                        # æœ‰äº›è¿”å›åœ¨ candidates åˆ—è¡¨ä¸­
                        if not text:
                            candidates = response.get("candidates") or response.get("outputs") or []
                            if candidates and isinstance(candidates, list):
                                first = candidates[0]
                                if isinstance(first, dict):
                                    # å¯èƒ½åœ¨ content/text å­—æ®µ
                                    text = first.get("content") or first.get("text") or first.get("output_text")
                                else:
                                    text = str(first)
                    # æœ€ç»ˆå›é€€ï¼šè½¬ä¸º str
                    if not text:
                        text = str(response)
                    full_response = text
                    placeholder.markdown(full_response)
                except Exception as e2:
                    # æŠ¥é”™æ—¶ç»™å‡ºæç¤º
                    st.error(f"è°ƒç”¨ Gemini å‡ºé”™ï¼š{e2}")
                    full_response = "[é”™è¯¯ï¼šæ— æ³•è·å¾—æ¨¡å‹å“åº”]"

            # ä¿å­˜ AI å“åº”
            st.session_state["messages"].append({
                "role": "assistant",
                "content": full_response
            })
else:
    st.chat_input("è¯·å…ˆåœ¨ä¾§è¾¹æ è¾“å…¥ Gemini API Key", disabled=True)
    st.warning("è¯·åœ¨ä¾§è¾¹æ é…ç½®ä½ çš„ Google Gemini API Key ä»¥å¼€å§‹èŠå¤©")
# 2. åˆå§‹åŒ–èŠå¤©è®°å½•ï¼ˆç”¨ Streamlit ä¼šè¯çŠ¶æ€å­˜å‚¨ï¼Œé¡µé¢åˆ·æ–°ä¸ä¸¢å¤±ï¼‰
if "messages" not in st.session_state:
    st.session_state.messages = []

# 3. æ˜¾ç¤ºå†å²èŠå¤©è®°å½•
for msg in st.session_state.messages:
    # æ”¯æŒè‹¥æ¶ˆæ¯å¸¦ attachments å­—æ®µåˆ™æ˜¾ç¤ºæ–‡ä»¶å
    with st.chat_message(msg["role"]):
        body = msg["content"]
        if msg.get("attachments"):
            body += "\n\n**é™„ä»¶:** " + ", ".join(msg["attachments"])
        st.markdown(body)

# 4. å¤„ç†ç”¨æˆ·è¾“å…¥å’Œ AI å“åº”
if api_key:
    # é…ç½® Gemini API
    configure(api_key=api_key)
    model = GenerativeModel(selected_model)

    # ä½¿ç”¨ form æŠŠè¾“å…¥ã€æ·»åŠ é™„ä»¶æŒ‰é’®å’Œå‘é€æŒ‰é’®æ”¾åœ¨åŒä¸€è¡Œï¼ˆå°½é‡æ¨¡æ‹Ÿâ€œå‘é€æ—è¾¹æœ‰æ·»åŠ é™„ä»¶æŒ‰é’®â€çš„å¸ƒå±€ï¼‰
    with st.form("chat_form", clear_on_submit=False):
        col_text, col_attach, col_send = st.columns([8, 1, 1])
        user_text = col_text.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜...", key="user_input", value="")
        # è¿™é‡Œä½œä¸ºâ€œæ·»åŠ é™„ä»¶â€æŒ‰é’®ï¼šaccept_multiple_files=Trueï¼Œè®©ç”¨æˆ·å¯ä»¥ä¸€æ¬¡é€‰å¤šä¸ªæ–‡ä»¶ã€‚
        # ä¸æŒ‡å®š type å‚æ•°å³æ¥å—ä»»æ„æ–‡ä»¶ç±»å‹ï¼ˆå›¾ç‰‡/éŸ³é¢‘/è§†é¢‘/æ–‡æœ¬ç­‰ï¼‰ã€‚
        files = col_attach.file_uploader("", accept_multiple_files=True, key="file_uploader", label_visibility="collapsed")
        send = col_send.form_submit_button("å‘é€")

    # å½“ç”¨æˆ·é€šè¿‡ file_uploader é€‰æ‹©æ–‡ä»¶æ—¶ï¼ŒæŠŠæ–‡ä»¶å¯¹è±¡å­˜å…¥ session_stateï¼Œä½†ä¸è¦è‡ªåŠ¨å‘é€
    if files:
        # ä¿ç•™å·²é€‰é™„ä»¶ï¼ˆUploadedFile å¯¹è±¡åˆ—è¡¨ï¼‰
        st.session_state["pending_attachments"] = files

    # æ˜¾ç¤ºå½“å‰å¾…å‘é€çš„é™„ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
    if st.session_state.get("pending_attachments"):
        pending = st.session_state["pending_attachments"]
        # æ˜¾ç¤ºæ–‡ä»¶åå’Œä¸€ä¸ªæ¸…é™¤æŒ‰é’®
        cols = st.columns([0.95, 0.05])
        cols[0].markdown("å·²é€‰é™„ä»¶: " + ", ".join([f.name for f in pending]))
        if cols[1].button("âœ– æ¸…é™¤é™„ä»¶"):
            st.session_state.pop("pending_attachments", None)

    # å½“ç”¨æˆ·ç‚¹å‡»å‘é€æŒ‰é’®ï¼ˆæˆ–è¡¨å•æäº¤ï¼‰æ—¶ï¼ŒæŠŠæ–‡æœ¬å’Œé™„ä»¶å…ƒæ•°æ®ä¸€èµ·åŠ å…¥ messages å¹¶è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›å¤
    if send and (user_text or st.session_state.get("pending_attachments")):
        attachments = st.session_state.pop("pending_attachments", [])
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯ï¼ˆè¿™é‡ŒæŠŠæ–‡ä»¶åä½œä¸ºé™„ä»¶å…ƒæ•°æ®å­˜å‚¨ï¼›å¦‚æœéœ€è¦æŠŠæ–‡ä»¶äºŒè¿›åˆ¶å‘ç»™ Geminiï¼Œéœ€è¦åœ¨è¿™é‡Œå¤„ç†ï¼‰
        st.session_state.messages.append({
            "role": "user",
            "content": user_text,
            "attachments": [f.name for f in attachments] if attachments else []
        })
        with st.chat_message("user"):
            display_text = user_text
            if attachments:
                display_text += "\n\n**é™„ä»¶:** " + ", ".join([f.name for f in attachments])
            st.markdown(display_text)

        # ç”Ÿæˆ AI å“åº”
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            # è°ƒç”¨ Gemini APIï¼ˆæµå¼å“åº”ï¼Œå®æ—¶æ˜¾ç¤ºï¼‰
            try:
                # æ³¨æ„ï¼šè‹¥ model.generate_content çš„å‚æ•°æ¥å£å’Œä½ æœ¬åœ° SDK ä¸åŒï¼ˆä¾‹å¦‚ä¸æ”¯æŒ streamï¼‰ï¼Œ
                # éœ€è¦æŒ‰ä½ çš„ SDK æ–‡æ¡£è°ƒæ•´è°ƒç”¨æ–¹å¼ã€‚è¿™é‡Œä¿ç•™åŸæ¥çš„æµå¼è°ƒç”¨ç¤ºä¾‹ã€‚
                response = model.generate_content(user_text, stream=True)
                for chunk in response:
                    if getattr(chunk, "text", None):
                        full_response += chunk.text
                        message_placeholder.markdown(full_response + "â–Œ")  # åŠ è½½åŠ¨ç”»
                message_placeholder.markdown(full_response)  # æœ€ç»ˆå“åº”
                # ä¿å­˜ AI å“åº”åˆ°ä¼šè¯çŠ¶æ€
                st.session_state.messages.append({"role": "assistant", "content": full_response})
            except Exception as e:
                st.error(f"API è°ƒç”¨å¤±è´¥ï¼š{str(e)}")
else:
    # æœªè¾“å…¥ API Key æ—¶æç¤º
    st.chat_input("è¯·å…ˆåœ¨ä¾§è¾¹æ è¾“å…¥ Gemini API Key", disabled=True)
    st.warning("è¯·åœ¨ä¾§è¾¹æ é…ç½®ä½ çš„ Google Gemini API Key ä»¥å¼€å§‹èŠå¤©")
