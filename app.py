import streamlit as st
from google.generativeai import GenerativeModel, configure

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="Gemini AI èŠå¤©",
    page_icon="ğŸ¤–",
    layout="wide"
)

# æ ‡é¢˜å’Œè¯´æ˜
st.title("ğŸ¤– Gemini AI èŠå¤©åŠ©æ‰‹")
st.caption("åŸºäº Google Gemini API çš„ç®€å•èŠå¤©å·¥å…·ï¼Œæ”¯æŒå¤šæ¨¡å‹é€‰æ‹©")

# 1. é…ç½® Gemini API Keyï¼ˆç”¨æˆ·éœ€åœ¨ä¾§è¾¹æ è¾“å…¥ï¼‰
with st.sidebar:
    st.header("ğŸ”§ é…ç½®")
    api_key = st.text_input("è¯·è¾“å…¥ä½ çš„ Google Gemini API Key", type="password")
    st.caption("API Key å¯ä» [Google AI Studio](https://aistudio.google.com/) è·å–")

    # æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†ï¼ˆé»˜è®¤ gemini-2.5-proï¼‰
    models = [
        "gemini-2.5-pro",
        "gemini-2.5-pro-latest",
        "gemini-1.5-pro",
        "gemini-1.5-flash",
        "gemini-pro"
    ]
    selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹", models, index=0)  # index=0 è®¾ä¸ºé»˜è®¤

    # æ¸…ç©ºèŠå¤©è®°å½•æŒ‰é’®
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºèŠå¤©è®°å½•"):
        st.session_state.messages = []
        # ä¹Ÿæ¸…é™¤å¾…å‘é€é™„ä»¶
        st.session_state.pop("pending_attachments", None)
        st.rerun()

# 2. åˆå§‹åŒ–èŠå¤©è®°å½•ï¼ˆç”¨ Streamlit ä¼šè¯çŠ¶æ€å­˜å‚¨ï¼Œé¡µé¢åˆ·æ–°ä¸ä¸¢å¤±ï¼‰
if "messages" not in st.session_state:
    st.session_state.messages = []

# 3. æ˜¾ç¤ºå†å²èŠå¤©è®°å½•
for msg in st.session_state.messages:
    # æ”¯æŒè‹¥æ¶ˆæ¯å¸¦ attachments å­—æ®µåˆ™æ˜¾ç¤ºæ–‡ä»¶å
    with st.chat_message(msg["role"]):
        body = msg["content"]
        if msg.get("attachments"):
            body += "\n\n**é™„ä»¶:** " + ", ".join(msg["attachments"])
        st.markdown(body)

# 4. å¤„ç†ç”¨æˆ·è¾“å…¥å’Œ AI å“åº”
if api_key:
    # é…ç½® Gemini API
    configure(api_key=api_key)
    model = GenerativeModel(selected_model)

    # ä½¿ç”¨ form æŠŠè¾“å…¥ã€æ·»åŠ é™„ä»¶æŒ‰é’®å’Œå‘é€æŒ‰é’®æ”¾åœ¨åŒä¸€è¡Œï¼ˆå°½é‡æ¨¡æ‹Ÿâ€œå‘é€æ—è¾¹æœ‰æ·»åŠ é™„ä»¶æŒ‰é’®â€çš„å¸ƒå±€ï¼‰
    with st.form("chat_form", clear_on_submit=False):
        col_text, col_attach, col_send = st.columns([8, 1, 1])
        user_text = col_text.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜...", key="user_input", value="")
        # è¿™é‡Œä½œä¸ºâ€œæ·»åŠ é™„ä»¶â€æŒ‰é’®ï¼šaccept_multiple_files=Trueï¼Œè®©ç”¨æˆ·å¯ä»¥ä¸€æ¬¡é€‰å¤šä¸ªæ–‡ä»¶ã€‚
        # ä¸æŒ‡å®š type å‚æ•°å³æ¥å—ä»»æ„æ–‡ä»¶ç±»å‹ï¼ˆå›¾ç‰‡/éŸ³é¢‘/è§†é¢‘/æ–‡æœ¬ç­‰ï¼‰ã€‚
        files = col_attach.file_uploader("", accept_multiple_files=True, key="file_uploader", label_visibility="collapsed")
        send = col_send.form_submit_button("å‘é€")

    # å½“ç”¨æˆ·é€šè¿‡ file_uploader é€‰æ‹©æ–‡ä»¶æ—¶ï¼ŒæŠŠæ–‡ä»¶å¯¹è±¡å­˜å…¥ session_stateï¼Œä½†ä¸è¦è‡ªåŠ¨å‘é€
    if files:
        # ä¿ç•™å·²é€‰é™„ä»¶ï¼ˆUploadedFile å¯¹è±¡åˆ—è¡¨ï¼‰
        st.session_state["pending_attachments"] = files

    # æ˜¾ç¤ºå½“å‰å¾…å‘é€çš„é™„ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
    if st.session_state.get("pending_attachments"):
        pending = st.session_state["pending_attachments"]
        # æ˜¾ç¤ºæ–‡ä»¶åå’Œä¸€ä¸ªæ¸…é™¤æŒ‰é’®
        cols = st.columns([0.95, 0.05])
        cols[0].markdown("å·²é€‰é™„ä»¶: " + ", ".join([f.name for f in pending]))
        if cols[1].button("âœ– æ¸…é™¤é™„ä»¶"):
            st.session_state.pop("pending_attachments", None)

    # å½“ç”¨æˆ·ç‚¹å‡»å‘é€æŒ‰é’®ï¼ˆæˆ–è¡¨å•æäº¤ï¼‰æ—¶ï¼ŒæŠŠæ–‡æœ¬å’Œé™„ä»¶å…ƒæ•°æ®ä¸€èµ·åŠ å…¥ messages å¹¶è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›å¤
    if send and (user_text or st.session_state.get("pending_attachments")):
        attachments = st.session_state.pop("pending_attachments", [])
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯ï¼ˆè¿™é‡ŒæŠŠæ–‡ä»¶åä½œä¸ºé™„ä»¶å…ƒæ•°æ®å­˜å‚¨ï¼›å¦‚æœéœ€è¦æŠŠæ–‡ä»¶äºŒè¿›åˆ¶å‘ç»™ Geminiï¼Œéœ€è¦åœ¨è¿™é‡Œå¤„ç†ï¼‰
        st.session_state.messages.append({
            "role": "user",
            "content": user_text,
            "attachments": [f.name for f in attachments] if attachments else []
        })
        with st.chat_message("user"):
            display_text = user_text
            if attachments:
                display_text += "\n\n**é™„ä»¶:** " + ", ".join([f.name for f in attachments])
            st.markdown(display_text)

        # ç”Ÿæˆ AI å“åº”
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            # è°ƒç”¨ Gemini APIï¼ˆæµå¼å“åº”ï¼Œå®æ—¶æ˜¾ç¤ºï¼‰
            try:
                # æ³¨æ„ï¼šè‹¥ model.generate_content çš„å‚æ•°æ¥å£å’Œä½ æœ¬åœ° SDK ä¸åŒï¼ˆä¾‹å¦‚ä¸æ”¯æŒ streamï¼‰ï¼Œ
                # éœ€è¦æŒ‰ä½ çš„ SDK æ–‡æ¡£è°ƒæ•´è°ƒç”¨æ–¹å¼ã€‚è¿™é‡Œä¿ç•™åŸæ¥çš„æµå¼è°ƒç”¨ç¤ºä¾‹ã€‚
                response = model.generate_content(user_text, stream=True)
                for chunk in response:
                    if getattr(chunk, "text", None):
                        full_response += chunk.text
                        message_placeholder.markdown(full_response + "â–Œ")  # åŠ è½½åŠ¨ç”»
                message_placeholder.markdown(full_response)  # æœ€ç»ˆå“åº”
                # ä¿å­˜ AI å“åº”åˆ°ä¼šè¯çŠ¶æ€
                st.session_state.messages.append({"role": "assistant", "content": full_response})
            except Exception as e:
                st.error(f"API è°ƒç”¨å¤±è´¥ï¼š{str(e)}")
else:
    # æœªè¾“å…¥ API Key æ—¶æç¤º
    st.chat_input("è¯·å…ˆåœ¨ä¾§è¾¹æ è¾“å…¥ Gemini API Key", disabled=True)
    st.warning("è¯·åœ¨ä¾§è¾¹æ é…ç½®ä½ çš„ Google Gemini API Key ä»¥å¼€å§‹èŠå¤©")
